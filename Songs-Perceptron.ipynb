{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning with an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to perform a perceptron learning algorithm\n",
    "1. Feed the features of the model that is required to be trained as input in the first layer.\n",
    "2. All weights and inputs will be multiplied – the multiplied result of each weight and input will be added up\n",
    "3. The Bias value will be added to shift the output function \n",
    "4. This value will be presented to the activation function (the type of activation function will depend on the need)\n",
    "5. The value received after the last step is the output value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Perceptron Diagram\n",
    "![](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output function\n",
    "$y = \\sum_{i=1}^{n} W_ix_i + b = W^Tx + b$\n",
    "##### Activation Function\n",
    "$y = \\sigma(W^Tx + b)$\n",
    "##### Decision Function\n",
    "![](https://pythonmachinelearning.pro/wp-content/ql-cache/quicklatex.com-bea8f3a6e8fb15cdf45b49ce21599a5b_l3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Krima\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import  shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515340</th>\n",
       "      <td>2006</td>\n",
       "      <td>51.28467</td>\n",
       "      <td>45.88068</td>\n",
       "      <td>22.19582</td>\n",
       "      <td>-5.53319</td>\n",
       "      <td>-3.61835</td>\n",
       "      <td>-16.36914</td>\n",
       "      <td>2.12652</td>\n",
       "      <td>5.18160</td>\n",
       "      <td>-8.66890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.81440</td>\n",
       "      <td>-3.75991</td>\n",
       "      <td>-30.92584</td>\n",
       "      <td>26.33968</td>\n",
       "      <td>-5.03390</td>\n",
       "      <td>21.86037</td>\n",
       "      <td>-142.29410</td>\n",
       "      <td>3.42901</td>\n",
       "      <td>-41.14721</td>\n",
       "      <td>-15.46052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515341</th>\n",
       "      <td>2006</td>\n",
       "      <td>49.87870</td>\n",
       "      <td>37.93125</td>\n",
       "      <td>18.65987</td>\n",
       "      <td>-3.63581</td>\n",
       "      <td>-27.75665</td>\n",
       "      <td>-18.52988</td>\n",
       "      <td>7.76108</td>\n",
       "      <td>3.56109</td>\n",
       "      <td>-2.50351</td>\n",
       "      <td>...</td>\n",
       "      <td>32.38589</td>\n",
       "      <td>-32.75535</td>\n",
       "      <td>-61.05473</td>\n",
       "      <td>56.65182</td>\n",
       "      <td>15.29965</td>\n",
       "      <td>95.88193</td>\n",
       "      <td>-10.63242</td>\n",
       "      <td>12.96552</td>\n",
       "      <td>92.11633</td>\n",
       "      <td>10.88815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515342</th>\n",
       "      <td>2006</td>\n",
       "      <td>45.12852</td>\n",
       "      <td>12.65758</td>\n",
       "      <td>-38.72018</td>\n",
       "      <td>8.80882</td>\n",
       "      <td>-29.29985</td>\n",
       "      <td>-2.28706</td>\n",
       "      <td>-18.40424</td>\n",
       "      <td>-22.28726</td>\n",
       "      <td>-4.52429</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.73598</td>\n",
       "      <td>-71.15954</td>\n",
       "      <td>-123.98443</td>\n",
       "      <td>121.26989</td>\n",
       "      <td>10.89629</td>\n",
       "      <td>34.62409</td>\n",
       "      <td>-248.61020</td>\n",
       "      <td>-6.07171</td>\n",
       "      <td>53.96319</td>\n",
       "      <td>-8.09364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515343</th>\n",
       "      <td>2006</td>\n",
       "      <td>44.16614</td>\n",
       "      <td>32.38368</td>\n",
       "      <td>-3.34971</td>\n",
       "      <td>-2.49165</td>\n",
       "      <td>-19.59278</td>\n",
       "      <td>-18.67098</td>\n",
       "      <td>8.78428</td>\n",
       "      <td>4.02039</td>\n",
       "      <td>-12.01230</td>\n",
       "      <td>...</td>\n",
       "      <td>67.16763</td>\n",
       "      <td>282.77624</td>\n",
       "      <td>-4.63677</td>\n",
       "      <td>144.00125</td>\n",
       "      <td>21.62652</td>\n",
       "      <td>-29.72432</td>\n",
       "      <td>71.47198</td>\n",
       "      <td>20.32240</td>\n",
       "      <td>14.83107</td>\n",
       "      <td>39.74909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515344</th>\n",
       "      <td>2005</td>\n",
       "      <td>51.85726</td>\n",
       "      <td>59.11655</td>\n",
       "      <td>26.39436</td>\n",
       "      <td>-5.46030</td>\n",
       "      <td>-20.69012</td>\n",
       "      <td>-19.95528</td>\n",
       "      <td>-6.72771</td>\n",
       "      <td>2.29590</td>\n",
       "      <td>10.31018</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.50511</td>\n",
       "      <td>-69.18291</td>\n",
       "      <td>60.58456</td>\n",
       "      <td>28.64599</td>\n",
       "      <td>-4.39620</td>\n",
       "      <td>-64.56491</td>\n",
       "      <td>-45.61012</td>\n",
       "      <td>-5.51512</td>\n",
       "      <td>32.35602</td>\n",
       "      <td>12.17352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515345 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
       "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
       "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
       "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
       "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
       "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
       "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
       "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
       "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
       "\n",
       "              7         8         9   ...        81         82         83  \\\n",
       "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
       "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
       "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
       "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
       "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
       "...          ...       ...       ...  ...       ...        ...        ...   \n",
       "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
       "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
       "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
       "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
       "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
       "\n",
       "               84        85        86         87        88         89  \\\n",
       "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
       "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
       "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
       "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
       "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
       "...           ...       ...       ...        ...       ...        ...   \n",
       "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
       "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
       "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
       "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
       "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
       "\n",
       "              90  \n",
       "0        2.26327  \n",
       "1       26.92061  \n",
       "2       -0.66345  \n",
       "3       18.85382  \n",
       "4       28.74903  \n",
       "...          ...  \n",
       "515340 -15.46052  \n",
       "515341  10.88815  \n",
       "515342  -8.09364  \n",
       "515343  39.74909  \n",
       "515344  12.17352  \n",
       "\n",
       "[515345 rows x 91 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the sonar dataset\n",
    "df = pd.read_csv('YearPredictionMSD.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_hot_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7bb3caee43db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Split values into two for two different nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'one_hot_encode' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(df.columns))\n",
    "X = df[df.columns[1:]].values\n",
    "y=df[df.columns[0]]\n",
    "#encode the dependent variable containing categorical values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "# Split values into two for two different nodes\n",
    "Y = one_hot_encoder(y)\n",
    "y, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data in training and testing\n",
    "# X,Y = shuffle(X,Y,random_state=1)\n",
    "# train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.20, random_state=42)\n",
    "train_x=X[:463715]\n",
    "test_x=X[463716:]\n",
    "train_y=Y[:463715]\n",
    "test_y=Y[463716:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and initialize the variables to work with the tensors\n",
    "learning_rate = 0.1\n",
    "training_epochs = 1000\n",
    " \n",
    "#Array to store cost obtained in each epoch\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input columns\n",
    "n_dim = X.shape[1]\n",
    "# Number of output columns\n",
    "n_class = 89\n",
    "\n",
    "# initialize placeholders for x\n",
    "x = tf.placeholder(tf.float32,[None,n_dim])\n",
    "# initialize matrix for a values, weights and bias as per x*w + b\n",
    "W = tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "b = tf.Variable(tf.zeros([n_class]))\n",
    "\n",
    "# initialize all variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# define the cost function\n",
    "y_ = tf.placeholder(tf.float32,[None,n_class])\n",
    "# output y = x*W + b\n",
    "# the softmax transforms them into values between 0 and 1\n",
    "# The softmax \"squishes\" the inputs so that sum(input) = 1 : it's a way of normalizing\n",
    "y = tf.nn.softmax(tf.matmul(x, W)+ b)\n",
    "# \n",
    "cost_function = tf.reduce_mean(-tf.reduce_sum((y_ * tf.log(y)),reduction_indices=[1]))\n",
    "# Step function used is the gredient descent to minimize the cost function\n",
    "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "mse_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0  -  cost:  50201.027\n",
      "epoch :  1  -  cost:  nan\n",
      "epoch :  2  -  cost:  nan\n",
      "epoch :  3  -  cost:  nan\n",
      "epoch :  4  -  cost:  nan\n",
      "epoch :  5  -  cost:  nan\n",
      "epoch :  6  -  cost:  nan\n",
      "epoch :  7  -  cost:  nan\n",
      "epoch :  8  -  cost:  nan\n",
      "epoch :  9  -  cost:  nan\n",
      "epoch :  10  -  cost:  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7f9e5aad4e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#calculate the cost for each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcost_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate the cost for each epoch\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(training_step,feed_dict={x:train_x,y_:train_y})\n",
    "    cost = sess.run(cost_function,feed_dict={x: train_x,y_: train_y})\n",
    "    cost_history = np.append(cost_history,cost)\n",
    "    print('epoch : ', epoch,  ' - ', 'cost: ', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = sess.run(y, feed_dict={x: test_x})\n",
    " \n",
    "#Calculate Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred_y,1), tf.argmax(test_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy\",sess.run(accuracy))\n",
    " \n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2978a4ebda29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
